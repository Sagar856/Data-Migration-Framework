{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f6401bc-ba63-43a1-9dda-74f63a985dc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a93a26f6-fc79-4ab6-a422-424a2cb37401",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parameters extraction\n",
    "Parameters = dbutils.widgets.get(\"Parameters\")\n",
    "Parameters = json.loads(Parameters)\n",
    "\n",
    "ProcessInstanceId = 0\n",
    "ProcessQueueId = 0\n",
    "StageId = 0\n",
    "TableName = \"\"\n",
    "\n",
    "for p in Parameters:\n",
    "    if p.get(\"TableName\") == \"Orders\":\n",
    "        ProcessInstanceId = int(p.get(\"ProcessInstanceId\"))\n",
    "        ProcessQueueId = int(p.get(\"ProcessQueueId\"))\n",
    "        StageId = int(p.get(\"StageId\"))\n",
    "        TableName = str(p.get(\"TableName\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bd25c27-19b1-40bc-b11f-db61624e9536",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mark current table as InProgress\n",
    "if StageId == 3:\n",
    "    spark.sql(f\"\"\"\n",
    "        update control.processqueue\n",
    "        set ProcessStatus = 'InProgress',\n",
    "            ProcessStartTime = current_timestamp()\n",
    "        where StageId = {StageId}\n",
    "            and ProcessInstanceId = {ProcessInstanceId}\n",
    "            and ProcessQueueId = {ProcessQueueId}\n",
    "            and TableName = '{TableName}';\n",
    "    \"\"\")\n",
    "else:\n",
    "    raise Exception(f\"Stage Id is not relavent to R2B-transformation for table: {TableName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54e2c148-768b-46f4-bb74-aa20813f1c5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Write tables to Silver\n",
    "status = False\n",
    "\n",
    "try:\n",
    "    # Read source (bronze) table\n",
    "    src_df = spark.table(\"workspace.bronze.orders\")\n",
    "\n",
    "    # Select, transform, and add computed columns\n",
    "    final_df = (\n",
    "        src_df.select(\n",
    "            \"OrderId\",\n",
    "            \"CustomerId\",\n",
    "            F.col(\"OrdersProductId\").alias(\"ProductId\"),\n",
    "            # Mirror CAST(OrderDate AS DATE). If the source is string, Spark will try to cast by the default date format.\n",
    "            F.col(\"OrderDate\").cast(\"date\").alias(\"OrderDate\"),\n",
    "            F.col(\"OrderPrice\").alias(\"OrderPrice\"),\n",
    "            F.current_timestamp().alias(\"LoadTimestamp\"),\n",
    "            F.lit(\"Source_CSV\").alias(\"SourceSystem\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Overwrite/create the Delta table at the target\n",
    "    (final_df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")             # mirrors CREATE OR REPLACE TABLE ... AS SELECT\n",
    "        .option(\"overwriteSchema\", \"true\")\n",
    "        .saveAsTable(\"workspace.silver.orders\"))\n",
    "    status = True\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    status = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cff6228c-d676-49f7-b9c8-27fa6cfd802d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mark file as Success/Failed\n",
    "\n",
    "if status == True:\n",
    "    spark.sql(f\"\"\"\n",
    "        UPDATE control.processqueue\n",
    "        SET\n",
    "            ProcessStatus = 'Succeeded',\n",
    "            ProcessEndTime = current_timestamp(),\n",
    "            ProcessDuration = CAST(\n",
    "                (unix_timestamp(current_timestamp()) - unix_timestamp(ProcessStartTime)) / 60\n",
    "                AS BIGINT\n",
    "            )\n",
    "        WHERE\n",
    "            StageId = {StageId}\n",
    "            AND ProcessInstanceId = {ProcessInstanceId}\n",
    "            AND ProcessQueueId = {ProcessQueueId}\n",
    "            AND TableName = '{TableName}'\n",
    "            \"\"\")\n",
    "    print(f\"{TableName} Marked as Successful\")\n",
    "elif status == False:\n",
    "        spark.sql(f\"\"\"\n",
    "        UPDATE control.processqueue\n",
    "        SET\n",
    "            ProcessStatus = 'Failed',\n",
    "            ProcessEndTime = current_timestamp(),\n",
    "            ProcessDuration = CAST(\n",
    "                (unix_timestamp(current_timestamp()) - unix_timestamp(ProcessStartTime)) / 60\n",
    "                AS BIGINT\n",
    "            )\n",
    "        WHERE\n",
    "            StageId = {StageId}\n",
    "            AND ProcessInstanceId = {ProcessInstanceId}\n",
    "            AND ProcessQueueId = {ProcessQueueId}\n",
    "            AND TableName = '{TableName}'\n",
    "            \"\"\")\n",
    "        print(f\"{TableName} Marked as Failed\")\n",
    "        raise Exception(f\"Hard failure: {TableName} Failure detected\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "B2S_Orders",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
