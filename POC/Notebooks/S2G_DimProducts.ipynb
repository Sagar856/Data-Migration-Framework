{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6aa416c9-fe54-49e2-a0fd-b6fb14263e9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import json\n",
    "from pyspark.sql.functions import col, monotonically_increasing_id\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    StringType, DoubleType, LongType\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d83441ab-3538-443f-9930-c6a3274acdc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parameters extraction\n",
    "Parameters = dbutils.widgets.get(\"Parameters\")\n",
    "Parameters = json.loads(Parameters)\n",
    "\n",
    "ProcessInstanceId = 0\n",
    "ProcessQueueId = 0\n",
    "StageId = 0\n",
    "TableName = \"\"\n",
    "\n",
    "for p in Parameters:\n",
    "    if p.get(\"TableName\") == \"Products\":\n",
    "        ProcessInstanceId = int(p.get(\"ProcessInstanceId\"))\n",
    "        ProcessQueueId = int(p.get(\"ProcessQueueId\"))\n",
    "        StageId = int(p.get(\"StageId\"))\n",
    "        TableName = str(p.get(\"TableName\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1dddbee7-f035-4960-aaa1-1ea417216c6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mark current table as InProgress\n",
    "if StageId == 4:\n",
    "    spark.sql(f\"\"\"\n",
    "        update control.processqueue\n",
    "        set ProcessStatus = 'InProgress',\n",
    "            ProcessStartTime = current_timestamp()\n",
    "        where StageId = {StageId}\n",
    "            and ProcessInstanceId = {ProcessInstanceId}\n",
    "            and ProcessQueueId = {ProcessQueueId}\n",
    "            and TableName = '{TableName}';\n",
    "    \"\"\")\n",
    "else:\n",
    "    raise Exception(f\"Stage Id is not relavent to R2B-transformation for table: {TableName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6528a985-4f86-4ff9-a886-d0d98323fd58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "status = False\n",
    "\n",
    "try: \n",
    "    # -------------------------------------------------\n",
    "    # Read Bronze Products\n",
    "    # -------------------------------------------------\n",
    "    products_df = spark.table(\"workspace.bronze.products\")\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Clean & standardize\n",
    "    # -------------------------------------------------\n",
    "    dim_product_df = (\n",
    "        products_df\n",
    "        .select(\n",
    "            col(\"ProductId\").cast(StringType()),\n",
    "            col(\"ProductName\").cast(StringType()),\n",
    "            col(\"ProductCategory\").cast(StringType()),\n",
    "            col(\"ProductPrice\").cast(DoubleType())\n",
    "        )\n",
    "        .dropDuplicates([\"ProductId\"])\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Add surrogate key\n",
    "    # -------------------------------------------------\n",
    "    dim_product_df = dim_product_df.withColumn(\n",
    "        \"ProductKey\",\n",
    "        monotonically_increasing_id()\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Define schema for Unknown Product\n",
    "    # -------------------------------------------------\n",
    "    unknown_schema = StructType([\n",
    "        StructField(\"ProductId\", StringType(), False),\n",
    "        StructField(\"ProductName\", StringType(), False),\n",
    "        StructField(\"ProductCategory\", StringType(), False),\n",
    "        StructField(\"ProductPrice\", DoubleType(), False),\n",
    "        StructField(\"ProductKey\", LongType(), False)\n",
    "    ])\n",
    "\n",
    "    unknown_product = spark.createDataFrame(\n",
    "        [(\"-1\", \"Unknown\", \"Unknown\", 0.0, -1)],\n",
    "        schema=unknown_schema\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Union Unknown record\n",
    "    # -------------------------------------------------\n",
    "    dim_product_df = dim_product_df.unionByName(unknown_product)\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Final column order\n",
    "    # -------------------------------------------------\n",
    "    dim_product_df = dim_product_df.select(\n",
    "        \"ProductKey\",\n",
    "        \"ProductId\",\n",
    "        \"ProductName\",\n",
    "        \"ProductCategory\",\n",
    "        \"ProductPrice\"\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Write to Gold\n",
    "    # -------------------------------------------------\n",
    "    (\n",
    "        dim_product_df\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .saveAsTable(\"workspace.gold.dim_product\")\n",
    "    )\n",
    "    status = True\n",
    "\n",
    "    # SCD-2 Implementation : ############################\n",
    "\n",
    "    from pyspark.sql.functions import *\n",
    "    from delta.tables import DeltaTable\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Source Data\n",
    "    # -------------------------------------------------\n",
    "    source_df = (\n",
    "        spark.table(\"workspace.bronze.products\")\n",
    "        .select(\n",
    "            col(\"ProductId\").cast(\"string\"),\n",
    "            col(\"ProductName\"),\n",
    "            col(\"ProductCategory\"),\n",
    "            col(\"ProductPrice\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"RecordHash\",\n",
    "            sha2(\n",
    "                concat_ws(\n",
    "                    \"||\",\n",
    "                    \"ProductName\",\n",
    "                    \"ProductCategory\",\n",
    "                    \"ProductPrice\"\n",
    "                ),\n",
    "                256\n",
    "            )\n",
    "        )\n",
    "        .withColumn(\"StartDate\", current_timestamp())\n",
    "        .withColumn(\"EndDate\", lit(None).cast(\"timestamp\"))\n",
    "        .withColumn(\"IsCurrent\", lit(True))\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Target Delta Table\n",
    "    # -------------------------------------------------\n",
    "    target = DeltaTable.forName(\n",
    "        spark,\n",
    "        \"workspace.gold.dim_product\"\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # SCD Type-2 MERGE (Corrected)\n",
    "    # -------------------------------------------------\n",
    "    (\n",
    "        target.alias(\"t\")\n",
    "        .merge(\n",
    "            source_df.alias(\"s\"),\n",
    "            \"t.ProductId = s.ProductId\"\n",
    "        )\n",
    "        # Expire existing record when data changes\n",
    "        .whenMatchedUpdate(\n",
    "            condition=\"t.IsCurrent = true AND t.RecordHash <> s.RecordHash\",\n",
    "            set={\n",
    "                \"EndDate\": \"current_timestamp()\",\n",
    "                \"IsCurrent\": \"false\"\n",
    "            }\n",
    "        )\n",
    "        # Insert new version\n",
    "        .whenNotMatchedInsert(\n",
    "            values={\n",
    "                \"ProductId\": \"s.ProductId\",\n",
    "                \"ProductName\": \"s.ProductName\",\n",
    "                \"ProductCategory\": \"s.ProductCategory\",\n",
    "                \"ProductPrice\": \"s.ProductPrice\",\n",
    "                \"StartDate\": \"s.StartDate\",\n",
    "                \"EndDate\": \"s.EndDate\",\n",
    "                \"IsCurrent\": \"s.IsCurrent\",\n",
    "                \"RecordHash\": \"s.RecordHash\"\n",
    "            }\n",
    "        )\n",
    "        .execute()\n",
    "    )\n",
    "    status = True\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    status = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0a7f82f-6abf-4ffb-a53b-d692a9e167d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mark file as Success/Failed\n",
    "\n",
    "if status == True:\n",
    "    spark.sql(f\"\"\"\n",
    "        UPDATE control.processqueue\n",
    "        SET\n",
    "            ProcessStatus = 'Succeeded',\n",
    "            ProcessEndTime = current_timestamp(),\n",
    "            ProcessDuration = CAST(\n",
    "                (unix_timestamp(current_timestamp()) - unix_timestamp(ProcessStartTime)) / 60\n",
    "                AS BIGINT\n",
    "            )\n",
    "        WHERE\n",
    "            StageId = {StageId}\n",
    "            AND ProcessInstanceId = {ProcessInstanceId}\n",
    "            AND ProcessQueueId = {ProcessQueueId}\n",
    "            AND TableName = '{TableName}'\n",
    "            \"\"\")\n",
    "    print(f\"{TableName} Marked as Successful\")\n",
    "elif status == False:\n",
    "        spark.sql(f\"\"\"\n",
    "        UPDATE control.processqueue\n",
    "        SET\n",
    "            ProcessStatus = 'Failed',\n",
    "            ProcessEndTime = current_timestamp(),\n",
    "            ProcessDuration = CAST(\n",
    "                (unix_timestamp(current_timestamp()) - unix_timestamp(ProcessStartTime)) / 60\n",
    "                AS BIGINT\n",
    "            )\n",
    "        WHERE\n",
    "            StageId = {StageId}\n",
    "            AND ProcessInstanceId = {ProcessInstanceId}\n",
    "            AND ProcessQueueId = {ProcessQueueId}\n",
    "            AND TableName = '{TableName}'\n",
    "            \"\"\")\n",
    "        print(f\"{TableName} Marked as Failed\")\n",
    "        raise Exception(f\"Hard failure: {TableName} Failure detected\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7298426064243796,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "S2G_DimProducts",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
