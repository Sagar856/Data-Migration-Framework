{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d8df1c5-e54f-422e-9c39-9a0a2e2f5c76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import json\n",
    "from pyspark.sql.functions import *\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.functions import col, lit, monotonically_increasing_id\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    IntegerType, StringType, DateType\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "786876f9-ba0a-4db3-8380-87a9c1c19cbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parameters extraction\n",
    "Parameters = dbutils.widgets.get(\"Parameters\")\n",
    "Parameters = json.loads(Parameters)\n",
    "\n",
    "ProcessInstanceId = 0\n",
    "ProcessQueueId = 0\n",
    "StageId = 0\n",
    "TableName = \"\"\n",
    "\n",
    "for p in Parameters:\n",
    "    if p.get(\"TableName\") == \"Customers\":\n",
    "        ProcessInstanceId = int(p.get(\"ProcessInstanceId\"))\n",
    "        ProcessQueueId = int(p.get(\"ProcessQueueId\"))\n",
    "        StageId = int(p.get(\"StageId\"))\n",
    "        TableName = str(p.get(\"TableName\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "642342d2-572b-41c0-8421-1ee451230ec0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mark current table as InProgress\n",
    "if StageId == 4:\n",
    "    spark.sql(f\"\"\"\n",
    "        update control.processqueue\n",
    "        set ProcessStatus = 'InProgress',\n",
    "            ProcessStartTime = current_timestamp()\n",
    "        where StageId = {StageId}\n",
    "            and ProcessInstanceId = {ProcessInstanceId}\n",
    "            and ProcessQueueId = {ProcessQueueId}\n",
    "            and TableName = '{TableName}';\n",
    "    \"\"\")\n",
    "else:\n",
    "    raise Exception(f\"Stage Id is not relavent to R2B-transformation for table: {TableName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d698824-a804-4cef-a390-0b67e4119c53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "status = False\n",
    "\n",
    "try: \n",
    "    # -------------------------------------------------\n",
    "    # Read Bronze Customers\n",
    "    # -------------------------------------------------\n",
    "    customers_df = spark.table(\"workspace.bronze.customers\")\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Clean & standardize\n",
    "    # -------------------------------------------------\n",
    "    dim_customer_df = (\n",
    "        customers_df\n",
    "        .select(\n",
    "            col(\"CustomerId\").cast(IntegerType()),\n",
    "            col(\"CustomerName\").cast(StringType()),\n",
    "            col(\"Age\").cast(IntegerType()),\n",
    "            col(\"Gender\").cast(StringType()),\n",
    "            col(\"City\").cast(StringType()),\n",
    "            col(\"JoinDate\").cast(DateType())\n",
    "        )\n",
    "        .dropDuplicates([\"CustomerId\"])\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Add surrogate key\n",
    "    # -------------------------------------------------\n",
    "    dim_customer_df = dim_customer_df.withColumn(\n",
    "        \"CustomerKey\",\n",
    "        monotonically_increasing_id()\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Define schema for Unknown Customer\n",
    "    # -------------------------------------------------\n",
    "    unknown_schema = StructType([\n",
    "        StructField(\"CustomerId\", IntegerType(), False),\n",
    "        StructField(\"CustomerName\", StringType(), False),\n",
    "        StructField(\"Age\", IntegerType(), False),\n",
    "        StructField(\"Gender\", StringType(), False),\n",
    "        StructField(\"City\", StringType(), False),\n",
    "        StructField(\"JoinDate\", DateType(), True),\n",
    "        StructField(\"CustomerKey\", IntegerType(), False)\n",
    "    ])\n",
    "\n",
    "    unknown_customer = spark.createDataFrame(\n",
    "        [(-1, \"Unknown\", -1, \"Unknown\", \"Unknown\", None, -1)],\n",
    "        schema=unknown_schema\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Union Unknown record\n",
    "    # -------------------------------------------------\n",
    "    dim_customer_df = dim_customer_df.unionByName(unknown_customer)\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Final column order\n",
    "    # -------------------------------------------------\n",
    "    dim_customer_df = dim_customer_df.select(\n",
    "        \"CustomerKey\",\n",
    "        \"CustomerId\",\n",
    "        \"CustomerName\",\n",
    "        \"Age\",\n",
    "        \"Gender\",\n",
    "        \"City\",\n",
    "        \"JoinDate\"\n",
    "    )\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # Write to Gold\n",
    "    # -------------------------------------------------\n",
    "    (\n",
    "        dim_customer_df\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .saveAsTable(\"workspace.gold.dim_customer\")\n",
    "    )\n",
    "    status = True\n",
    "\n",
    "\n",
    "# SCD-2 Implementation : ############################\n",
    "    # ---------------------------------------\n",
    "    # Source\n",
    "    # ---------------------------------------\n",
    "    source_df = (\n",
    "        spark.table(\"workspace.bronze.customers\")\n",
    "        .select(\n",
    "            col(\"CustomerId\").cast(\"string\"),\n",
    "            col(\"CustomerName\"),\n",
    "            col(\"Age\"),\n",
    "            col(\"Gender\"),\n",
    "            col(\"City\")\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"RecordHash\",\n",
    "            sha2(concat_ws(\"||\", \"CustomerName\", \"Age\", \"Gender\", \"City\"), 256)\n",
    "        )\n",
    "        .withColumn(\"StartDate\", current_timestamp())\n",
    "        .withColumn(\"EndDate\", lit(None).cast(\"timestamp\"))\n",
    "        .withColumn(\"IsCurrent\", lit(True))\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # Target\n",
    "    # ---------------------------------------\n",
    "    target = DeltaTable.forName(\n",
    "        spark,\n",
    "        \"workspace.gold.dim_customer\"\n",
    "    )\n",
    "\n",
    "    # ---------------------------------------\n",
    "    # SCD-2 MERGE\n",
    "    # ---------------------------------------\n",
    "    (\n",
    "        target.alias(\"t\")\n",
    "        .merge(\n",
    "            source_df.alias(\"s\"),\n",
    "            \"t.CustomerId = s.CustomerId\"\n",
    "        )\n",
    "        # Expire current record if changed\n",
    "        .whenMatchedUpdate(\n",
    "            condition=\"t.IsCurrent = true AND t.RecordHash <> s.RecordHash\",\n",
    "            set={\n",
    "                \"EndDate\": \"current_timestamp()\",\n",
    "                \"IsCurrent\": \"false\"\n",
    "            }\n",
    "        )\n",
    "        # Insert new version\n",
    "        .whenNotMatchedInsert(\n",
    "            values={\n",
    "                \"CustomerId\": \"s.CustomerId\",\n",
    "                \"CustomerName\": \"s.CustomerName\",\n",
    "                \"Age\": \"s.Age\",\n",
    "                \"Gender\": \"s.Gender\",\n",
    "                \"City\": \"s.City\",\n",
    "                \"StartDate\": \"s.StartDate\",\n",
    "                \"EndDate\": \"s.EndDate\",\n",
    "                \"IsCurrent\": \"s.IsCurrent\",\n",
    "                \"RecordHash\": \"s.RecordHash\"\n",
    "            }\n",
    "        )\n",
    "        .execute()\n",
    "    )\n",
    "    status = True\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    status = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74b422ca-7309-4c0b-8a77-92857c82cbf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mark file as Success/Failed\n",
    "\n",
    "if status == True:\n",
    "    spark.sql(f\"\"\"\n",
    "        UPDATE control.processqueue\n",
    "        SET\n",
    "            ProcessStatus = 'Succeeded',\n",
    "            ProcessEndTime = current_timestamp(),\n",
    "            ProcessDuration = CAST(\n",
    "                (unix_timestamp(current_timestamp()) - unix_timestamp(ProcessStartTime)) / 60\n",
    "                AS BIGINT\n",
    "            )\n",
    "        WHERE\n",
    "            StageId = {StageId}\n",
    "            AND ProcessInstanceId = {ProcessInstanceId}\n",
    "            AND ProcessQueueId = {ProcessQueueId}\n",
    "            AND TableName = '{TableName}'\n",
    "            \"\"\")\n",
    "    print(f\"{TableName} Marked as Successful\")\n",
    "elif status == False:\n",
    "        spark.sql(f\"\"\"\n",
    "        UPDATE control.processqueue\n",
    "        SET\n",
    "            ProcessStatus = 'Failed',\n",
    "            ProcessEndTime = current_timestamp(),\n",
    "            ProcessDuration = CAST(\n",
    "                (unix_timestamp(current_timestamp()) - unix_timestamp(ProcessStartTime)) / 60\n",
    "                AS BIGINT\n",
    "            )\n",
    "        WHERE\n",
    "            StageId = {StageId}\n",
    "            AND ProcessInstanceId = {ProcessInstanceId}\n",
    "            AND ProcessQueueId = {ProcessQueueId}\n",
    "            AND TableName = '{TableName}'\n",
    "            \"\"\")\n",
    "        print(f\"{TableName} Marked as Failed\")\n",
    "        raise Exception(f\"Hard failure: {TableName} Failure detected\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7298426064243782,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "S2G_DimCustomers",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
