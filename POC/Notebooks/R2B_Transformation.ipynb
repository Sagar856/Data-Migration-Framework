{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd9a5ae1-8dbb-4291-b8af-e5ddd394b528",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import (\n",
    "    col, lit, expr, to_timestamp,\n",
    "    sum as spark_sum\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6816a48d-347d-49ed-9025-928271fe698f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbutils.widgets.text(\"TableName\", \"\")\n",
    "# dbutils.widgets.text(\"StageId\", \"\")\n",
    "# dbutils.widgets.text(\"ProcessInstanceId\", \"\")\n",
    "# dbutils.widgets.text(\"ProcessQueueId\", \"\")\n",
    "\n",
    "# TableName = dbutils.widgets.get(\"TableName\")\n",
    "# StageId = (dbutils.widgets.get(\"StageId\"))\n",
    "# ProcessInstanceId = (dbutils.widgets.get(\"ProcessInstanceId\"))\n",
    "# ProcessQueueId = (dbutils.widgets.get(\"ProcessQueueId\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31db7263-ab7b-43f3-912d-132644d6c46f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "StageId = int(dbutils.widgets.get(\"StageId\"))\n",
    "ProcessInstanceId = int(dbutils.widgets.get(\"ProcessInstanceId\"))\n",
    "ProcessQueueId = int(dbutils.widgets.get(\"ProcessQueueId\"))\n",
    "TableName = dbutils.widgets.get(\"TableName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "787e9613-639e-4109-8009-d2e6a1d4e38b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mark current table as InProgress\n",
    "if StageId == 2:\n",
    "    spark.sql(f\"\"\"\n",
    "        update control.processqueue\n",
    "        set ProcessStatus = 'InProgress',\n",
    "            ProcessStartTime = current_timestamp()\n",
    "        where StageId = {StageId}\n",
    "            and ProcessInstanceId = {ProcessInstanceId}\n",
    "            and ProcessQueueId = {ProcessQueueId}\n",
    "            and TableName = '{TableName}';\n",
    "    \"\"\")\n",
    "else:\n",
    "    raise Exception(f\"Stage Id is not relavent to R2B-transformation for table: {TableName}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e879da3b-37ea-41fc-a24f-ea7c12f13797",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rawdf = (\n",
    "    spark.read\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(f\"/Volumes/workspace/raw/raw-volume/{TableName}/{TableName}.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02d028dc-caec-455a-9bb8-36e27e941ad8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Status = False\n",
    "\n",
    "bronze_path = f\"/Volumes/workspace/bronze/\"\n",
    "\n",
    "# -------------------------------------------------\n",
    "# READ METADATA\n",
    "# -------------------------------------------------\n",
    "metadata_df = (\n",
    "    spark.table(\"workspace.metadata.TableColumnDetails\")\n",
    "    .filter(col(\"TableName\") == TableName)\n",
    ")\n",
    "\n",
    "metadata_cols = [r.ColumnName for r in metadata_df.collect()]\n",
    "try:\n",
    "    # -------------------------------------------------\n",
    "    # COLUMN NAME NORMALIZATION & ALIGNMENT\n",
    "    # -------------------------------------------------\n",
    "    def normalize(c):\n",
    "        return c.replace(\" \", \"\").replace(\"_\", \"\").lower()\n",
    "\n",
    "    mapping = {}\n",
    "    for raw_col in rawdf.columns:\n",
    "        for meta_col in metadata_cols:\n",
    "            if normalize(raw_col) == normalize(meta_col):\n",
    "                mapping[raw_col] = meta_col\n",
    "                break\n",
    "\n",
    "    for raw_col, meta_col in mapping.items():\n",
    "        rawdf = rawdf.withColumnRenamed(raw_col, meta_col)\n",
    "\n",
    "    # Drop extra columns\n",
    "    rawdf = rawdf.select([c for c in rawdf.columns if c in metadata_cols])\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # SCHEMA ENFORCEMENT + DATE HANDLING (FIXED)\n",
    "    # -------------------------------------------------\n",
    "    for row in metadata_df.collect():\n",
    "        col_name = row.ColumnName\n",
    "        spark_type = row.DataType.lower()\n",
    "\n",
    "        if col_name not in rawdf.columns:\n",
    "            # Add missing column\n",
    "            rawdf = rawdf.withColumn(\n",
    "                col_name,\n",
    "                lit(None).cast(spark_type)\n",
    "            )\n",
    "        else:\n",
    "            if spark_type == \"timestamp\":\n",
    "                # ✅ Correct date parsing (DD/MM/YYYY)\n",
    "                rawdf = rawdf.withColumn(\n",
    "                col_name,\n",
    "                expr(f\"try_to_timestamp({col_name}, 'dd/MM/yyyy')\")\n",
    "                )\n",
    "            else:\n",
    "                # Safe cast for all other datatypes\n",
    "                rawdf = rawdf.withColumn(\n",
    "                    col_name,\n",
    "                    expr(f\"try_cast({col_name} AS {spark_type})\")\n",
    "                )\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # NULL VALIDATION (PK + NOT NULL)\n",
    "    # -------------------------------------------------\n",
    "    critical_cols = [\n",
    "        r.ColumnName\n",
    "        for r in metadata_df\n",
    "            .filter((col(\"IsNullable\") == 0) | (col(\"IsPrimaryKey\") == \"Y\"))\n",
    "            .select(\"ColumnName\")\n",
    "            .toLocalIterator()\n",
    "    ]\n",
    "\n",
    "    if critical_cols:\n",
    "        null_exprs = [\n",
    "            spark_sum(col(c).isNull().cast(\"int\")).alias(c)\n",
    "            for c in critical_cols\n",
    "        ]\n",
    "\n",
    "        null_counts = rawdf.select(null_exprs).collect()[0].asDict()\n",
    "        violations = [c for c, cnt in null_counts.items() if cnt > 0]\n",
    "\n",
    "        if violations:\n",
    "            raise Exception(\n",
    "                f\"Null values found in non-nullable columns: {violations}\"\n",
    "            )\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # DUPLICATE REMOVAL (PRIMARY KEY BASED)\n",
    "    # -------------------------------------------------\n",
    "    pk_cols = [\n",
    "        r.ColumnName\n",
    "        for r in metadata_df\n",
    "            .filter(col(\"IsPrimaryKey\") == \"Y\")\n",
    "            .select(\"ColumnName\")\n",
    "            .toLocalIterator()\n",
    "    ]\n",
    "\n",
    "    if pk_cols:\n",
    "        rawdf = rawdf.dropDuplicates(pk_cols)\n",
    "\n",
    "    # -------------------------------------------------\n",
    "    # WRITE TO BRONZE\n",
    "    # -------------------------------------------------\n",
    "    bronze_table = f\"workspace.bronze.{TableName}\"\n",
    "    (\n",
    "        rawdf.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")   # or overwrite\n",
    "        .saveAsTable(bronze_table)\n",
    "    )\n",
    "    status = True\n",
    "    print(f\"✅ Transformation completed for table: {TableName}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    status = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85bdb639-557b-4675-899a-a0759bed242e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Mark file as Success/Failed\n",
    "if status == True:\n",
    "    spark.sql(f\"\"\"\n",
    "        UPDATE control.processqueue\n",
    "        SET\n",
    "            ProcessStatus = 'Succeeded',\n",
    "            ProcessEndTime = current_timestamp(),\n",
    "            ProcessDuration = CAST(\n",
    "                (unix_timestamp(current_timestamp()) - unix_timestamp(ProcessStartTime)) / 60\n",
    "                AS BIGINT\n",
    "            )\n",
    "        WHERE\n",
    "            StageId = {StageId}\n",
    "            AND ProcessInstanceId = {ProcessInstanceId}\n",
    "            AND ProcessQueueId = {ProcessQueueId}\n",
    "            AND TableName = '{TableName}'\n",
    "            \"\"\")\n",
    "    print(f\"{TableName} Marked as Successful\")\n",
    "elif status == False:\n",
    "        spark.sql(f\"\"\"\n",
    "        UPDATE control.processqueue\n",
    "        SET\n",
    "            ProcessStatus = 'Failed',\n",
    "            ProcessEndTime = current_timestamp(),\n",
    "            ProcessDuration = CAST(\n",
    "                (unix_timestamp(current_timestamp()) - unix_timestamp(ProcessStartTime)) / 60\n",
    "                AS BIGINT\n",
    "            )\n",
    "        WHERE\n",
    "            StageId = {StageId}\n",
    "            AND ProcessInstanceId = {ProcessInstanceId}\n",
    "            AND ProcessQueueId = {ProcessQueueId}\n",
    "            AND TableName = '{TableName}'\n",
    "            \"\"\")\n",
    "        print(f\"{TableName} Marked as Failed\")\n",
    "        raise Exception(f\"Hard failure: {TableName} Failure detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be3a072a-dba1-4f28-865a-e94649d754fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "R2B_Transformation",
   "widgets": {
    "ProcessInstanceId": {
     "currentValue": "10003",
     "nuid": "2c669ccb-e921-4778-9a11-71b21489245d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "ProcessInstanceId",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "ProcessInstanceId",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "ProcessQueueId": {
     "currentValue": "1005",
     "nuid": "bbc8b11f-2ef4-424c-b1f8-094bfa7ced45",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "ProcessQueueId",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "ProcessQueueId",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "StageId": {
     "currentValue": "2",
     "nuid": "3b0e3f44-bed3-451a-bfc2-75d2e6637306",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "StageId",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "StageId",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "TableName": {
     "currentValue": "Orders",
     "nuid": "96652678-9848-4fe5-ae61-a5972acc5584",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "TableName",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "TableName",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
